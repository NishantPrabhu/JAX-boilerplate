
# Test configuration to see if the code is working with TPUs
# Training ResNet18 for 200 epochs on CIFAR10 dataset

epochs: 200
eval_every: 10
log_every: 50                                       # Logging every n "local" steps

data:
  name: cifar10
  batch_size: 128                                   # This batch size is per device
  num_workers: 2
  transforms:
    train:
      random_resized_crop:
        size: [32, 32]
      color_jitter:
        brightness: 0.4
        saturation: 0.4
        contrast: 0.4
        hue: 0.1
        apply_prob: 0.8
      random_gray:
        apply_prob: 0.2
      random_flip:
      to_tensor:
      normalize:
        mean: [0.4914, 0.4822, 0.4465]
        std: [0.2470, 0.2435, 0.2616]
    test:
      center_crop:
        size: [32, 32]
      to_tensor:
      normalize:
        mean: [0.4914, 0.4822, 0.4465]
        std: [0.2470, 0.2435, 0.2616]

optim:
  name: sgd
  momentum: 0.9
  nesterov: True

scheduler:
  name: cosine
  init_value: 0.1
  alpha: 0.0

checkpoint:
  metric: accuracy
  worst_value: 0.0

wandb:
  project: jax-tpu-test
